PROJECT Lakehouse ADF + Databricks + ADLS (Raw/Silver/Gold)

Statut : dÃ©pÃ´t public de dÃ©monstration â€” aucun secret, infra non active (subscription Azure dÃ©sactivÃ©e).

ğŸ¯ RÃ©sumÃ©

Automatisation de bout en bout dâ€™un flux CSV â†’ Delta dans un Data Lakehouse (zones Raw/Silver/Gold) avec Azure Data Factory (ADF) pour lâ€™orchestration et Azure Databricks pour le traitement.
DÃ©clenchement event-driven (Storage Events), nettoyage & normalisation, idempotence et monitoring.

âœ¨ Points forts

Event-driven : dÃ©pÃ´t dâ€™un .csv â‡’ pipeline ADF auto.

Delta Lake partout (silver/gold) pour fiabilitÃ© & ACID.

Nommage colonnes sanitizÃ© (compat Delta) âœ…

Idempotence : append en bronze, overwrite contrÃ´lÃ© en clean/curated.

Infra simple â†’ trajectoire Managed Identity / Access Connector.

ObservabilitÃ© : ADF Monitor (triggers, activities) + contrÃ´les Spark.

ğŸ—ï¸ Architecture (vue rapide)
graph LR
A[ADLS Gen2 - Container RAW] -- Storage Event (.csv) --> T[Event Grid]
T --> P[Azure Data Factory - Pipeline]
P --> N1[Databricks Notebook - Ingest_Raw]
N1 --> B[Silver/bronze (Delta)]
P --> N2[Databricks Notebook - Clean_Data]
N2 --> S[Silver/clean (Delta)]
P --> N3[Databricks Notebook - Transform_curated]
N3 --> G[Gold/curated (Delta)]


Compute : Databricks DBR 16.4 LTS (Spark 3.5.2)
Stockage : stmystack4858 (containers raw, silver, gold)
DÃ©clenchement : Event Grid (filtre *.csv sur raw/)

ğŸ—‚ï¸ Contenu du dÃ©pÃ´t
adf/
  arm_template/
    ARMTemplateForFactory.json
    ARMTemplateParametersForFactory.json
databricks/
  notebooks/
    Ingest_Raw.py
    Clean_Data.py
    Transform_curated.py
config/
  dev.yaml                     # ParamÃ¨tres non sensibles (ex: chemins abfss)
docs/
  rapport.md                   # Contexte, archi, incidents & runbook
  deploiement.md               # (si prÃ©sent) guide de dÃ©ploiement
.gitattributes, .gitignore, .editorconfig, LICENSE (si prÃ©sent)
adf/arm_template/parameters.local.json.example  # Exemple (pas de secrets)

ğŸ”§ DÃ©tails clÃ©s
ADF â€” ParamÃ¨tres & mapping du trigger

ParamÃ¨tres pipeline : storageAccount, rawContainer, silverContainer, goldContainer, fullPathAbfss.

Mapping Storage Events â†’ pipeline :

@concat(triggerBody().folderPath, '/', triggerBody().fileName)


Ã‰criture :

abfss://silver@.../bronze

abfss://gold@.../curated

Databricks â€” Notebooks

Ingest_Raw : lecture CSV (raw) â†’ sanitization colonnes â†’ Delta (silver/bronze)

Clean_Data : bronze â†’ transformations â€œcleanâ€ (types, nulls, qualitÃ©)

Transform_curated : clean â†’ agrÃ©gations / business rules â†’ gold/curated

Exemple de sanitization (extrait) :

def sanitize_col(c: str) -> str:
    return (c.strip()
             .lower()
             .replace(" ", "_")
             .replace("(", "")
             .replace(")", "")
             .replace("/", "_"))

df = df.toDF(*[sanitize_col(c) for c in df.columns])

ğŸ”’ SÃ©curitÃ© (repo public)

Aucun secret dans Git (tokens, clÃ©s). Les fichiers sensibles sont exclus (.gitignore) et remplacÃ©s par des placeholders (parameters.local.json.example).

Secret scanning / Dependabot activÃ©s sur GitHub (recommandÃ©).

ğŸ“Š RÃ©sultats (exemple Ã  adapter)

Pipeline entiÃ¨rement automatisÃ© (event-driven)

Latence moyenne < N minutes entre dÃ©pÃ´t et gold

N fichiers/jour traitÃ©s, M colonnes normalisÃ©es, 0 opÃ©rations manuelles

Remplacer N/M par vos mÃ©triques rÃ©elles.

ğŸ›£ï¸ Roadmap

AccÃ¨s ADLS via Managed Identity / Access Connector (UC)

Auto Loader + checkpoints (ingestion incrÃ©mentale robuste)

Data Quality (Great Expectations) + tests PySpark

Partitionnement par date (/ingest_date=YYYY-MM-DD) en Silver/Gold

Exposition BI : Unity Catalog (tables externes/volumes) + Power BI

ğŸ“š Pour en savoir plus

docs/rapport.md : Contexte & objectifs, archi cible, incidents & remÃ©diations, runbook.

docs/deploiement.md : Guide de dÃ©ploiement ARM + import notebooks (si prÃ©sent).

ğŸ‘¤ Auteur

Amine S. â€” Data Engineer
Projet vitrine : Azure Data Factory â€¢ Databricks â€¢ ADLS â€¢ Delta Lake â€¢ Event-driven pipelines
